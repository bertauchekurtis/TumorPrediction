nrounds = 10000,
nthreads = 28,
print_every_n = 2500,
early_stopping_rounds = 2,
data = trainingxgMatrix)
bestModel2 <- xgboost(booster = "gbtree",
objective = "reg:squarederror",
gamma = 0.3,
child_weight = 1,
max_depth = 9,
subsample = 0.8,
col_subsample = 1,
eta = 0.01,
nrounds = 10000,
nthreads = 28,
print_every_n = 2500,
early_stopping_rounds = 2,
data = training2xgMatrix)
predictions <- predict(bestModel, testingxgMatrix)
residual <- testingDataLabels - predictions
print("RMSE:") # RMSE calculation
sqrt(mean(((residual))^2))
print("MAE:")# MAE calculation
mean(abs(residual))
# calculate 95% error window size
q <- quantile(residual, probs =c(.025,.975))
abs(q[1]) + abs(q[2]) # total length of window
cor(testingDataLabels, predictions)
predictions2 <- predict(bestModel2, testing2xgMatrix)
residual2 <- testing2DataLabels - predictions2
print("RMSE:") # RMSE calculation
sqrt(mean(((residual2))^2))
print("MAE:")# MAE calculation
mean(abs(residual2))
data <- read.csv(file = "C:/Users/Kurtis/Desktop/Research/data/RetentionTime_HCD_Marx2013_SuppT3.csv")
set.seed(37)
library(stringr)
library(xgboost)
# predictor variables
encode = function(data)
{
data$peptideLength <- nchar(data$Peptide.Sequence2)
data$unmodA <- str_count(data$Peptide.Sequence2, "A")
data$unmodC <- str_count(data$Peptide.Sequence2, "C")
data$unmodD <- str_count(data$Peptide.Sequence2, "D")
data$unmodE <- str_count(data$Peptide.Sequence2, "E")
data$unmodF <- str_count(data$Peptide.Sequence2, "F")
data$unmodG <- str_count(data$Peptide.Sequence2, "G")
data$unmodH <- str_count(data$Peptide.Sequence2, "H")
data$unmodI <- str_count(data$Peptide.Sequence2, "I")
data$unmodK <- str_count(data$Peptide.Sequence2, "K")
data$unmodL <- str_count(data$Peptide.Sequence2, "L")
data$unmodM <- str_count(data$Peptide.Sequence2, "M")
data$unmodN <- str_count(data$Peptide.Sequence2, "N")
data$unmodP <- str_count(data$Peptide.Sequence2, "P")
data$unmodQ <- str_count(data$Peptide.Sequence2, "Q")
data$unmodR <- str_count(data$Peptide.Sequence2, "R")
data$unmodS <- str_count(data$Peptide.Sequence2, "S")
data$unmodT <- str_count(data$Peptide.Sequence2, "T")
data$unmodV <- str_count(data$Peptide.Sequence2, "V")
data$unmodW <- str_count(data$Peptide.Sequence2, "W")
data$unmodY <- str_count(data$Peptide.Sequence2, "Y")
data$modS <- str_count(data$Peptide.Sequence2, "s")
data$modT <- str_count(data$Peptide.Sequence2, "t")
data$modY <- str_count(data$Peptide.Sequence2, "y")
data$modM <- str_count(data$Peptide.Sequence2, "m")
data
}
data <- encode(data)
# remove non-numeric data
data$Peptide.Sequence2 <- NULL
# split data into training/testing sets
setAssignments <- sample(1:2, size = nrow(data), prob = c(0.8, 0.2), replace = TRUE)
trainingData <- data[setAssignments == 1,]
testingData <- data[setAssignments == 2,]
# exract retention times (output) from data
trainingRetentionTimesLabels <- trainingData$RetentionTime
trainingData$RetentionTime <- NULL
# xg boost matrix creation
trainingxgMatrix <- xgb.DMatrix(data.matrix(trainingData), label = trainingRetentionTimesLabels)
# for testing
testingDataLabels <- testingData$RetentionTime
testingData$RetentionTime <- NULL
testingxgMatrix <- xgb.DMatrix(data.matrix(testingData), label = testingDataLabels)
# analyze things
set.seed(37)
bestModel <-xgboost(booster = "gbtree",
objective = "reg:squarederror",
gamma = 0.2,
child_weight = 1,
max_depth = 11,
subsample = 0.9,
col_subsample = 1,
eta = 0.01,
nrounds = 10000,
nthreads = 28,
print_every_n = 2500,
early_stopping_rounds = 2,
data = trainingxgMatrix)
predictions <- predict(bestModel, testingxgMatrix)
residual <- testingDataLabels - predictions
print("RMSE:") # RMSE calculation
sqrt(mean(((residual))^2))
print("MAE:")# MAE calculation
mean(abs(residual))
# calculate 95% error window size
q <- quantile(residual, probs =c(.025,.975))
abs(q[1]) + abs(q[2]) # total length of window
cor(testingDataLabels, predictions)
print("RMSE:") # RMSE calculation
sqrt(mean(((residual))^2))
print("MAE:")# MAE calculation
mean(abs(residual))
# calculate 95% error window size
q <- quantile(residual, probs =c(.025,.975))
abs(q[1]) + abs(q[2]) # total length of window
cor(testingDataLabels, predictions)
# PREDICTIONS - TRAIN DATA ONE TEST DATA TWO
predictions_m1d2 <- predict(bestModel, testing2xgMatrix)
residual_m1d2 <- testing2DataLabels - predictions_m1d2
print("RMSE:")
sqrt(mean((residual_m1d2))^2)
print("MAE:")
mean(abs(residual_m1d2))
q <- quantile(residual_m1d2, probs = c(.025, .975))
abs(q[1]) + abs(q[2])
cor(testing2DataLabels, predictions_m1d2)
# PREDICTIONS - TRAIN DATA ONE TEST DATA TWO
predictions_m1d2 <- predict(bestModel, testing2xgMatrix)
residual_m1d2 <- testing2DataLabels - predictions_m1d2
print("RMSE:")
sqrt(mean((residual_m1d2))^2)
print("MAE:")
mean(abs(residual_m1d2))
q <- quantile(residual_m1d2, probs = c(.025, .975))
abs(q[1]) + abs(q[2])
set.seed(37)
# PREDICTIONS - TRAIN DATA 2 TEST DATA 2
predictions2 <- predict(bestModel2, testing2xgMatrix)
residual2 <- testing2DataLabels - predictions2
print("RMSE:") # RMSE calculation
sqrt(mean(((residual2))^2))
print("MAE:")# MAE calculation
mean(abs(residual2))
# calculate 95% error window sizes
q <- quantile(residual2, probs =c(.025,.975))
abs(q[1]) + abs(q[2]) # total length of window
cor(testing2DataLabels, predictions2)
# PREDICTIONS - TRAIN DATA 2 TEST DATA 1
predictions_m2d1 <- predict(bestModel2, testingxgMatrix)
residual_m2d1 <- testingDataLabels - predictions_m2d1
print("RMSE:") # RMSE calculation
sqrt(mean(((residual_m2d1))^2))
print("MAE:")# MAE calculation
mean(abs(residual_m2d1))
# calculate 95% error window sizes
q <- quantile(residual_m2d1, probs =c(.025,.975))
abs(q[1]) + abs(q[2]) # total length of window
cor(testingDataLabels, predictions_m2d1)
sqrt(mean((residual_m1d2))^2)
residual_m2d1 <- testingDataLabels - predictions_m2d1
print("RMSE:") # RMSE calculation
sqrt(mean(((residual_m2d1))^2))
residual2 <- testing2DataLabels - predictions2
print("RMSE:") # RMSE calculation
sqrt(mean(((residual2))^2))
# PREDICTIONS - TRAIN DATA ONE TEST DATA TWO
predictions_m1d2 <- predict(bestModel, testing2xgMatrix)
residual_m1d2 <- testing2DataLabels - predictions_m1d2
print("RMSE:")
sqrt(mean((residual_m1d2))^2)
modelA <- bestModel
dataB_xgMat_train <- training2xgMatrix
predicts_of_train_B_from_model_A <- predict(modelA, dataB_xgbMat_train)
dataB_xgbMat_train <- training2xgMatrix
predicts_of_train_B_from_model_A <- predict(modelA, dataB_xgbMat_train)
length(predicts_of_train_B_from_model_A)
dataB_train_labels <- training2RetentionTimesLabels
dataB_test_labels <- testing2DataLabels
df <- data.frame(dataB_train_labels, predicts_of_train_B_from_model_A)
newLM <- lm(dataB_train_labels ~ predicts_of_train_B_from_model_A, data = df)
predicts_of_test_B_from_model_A <- predict(modelA, dataB_xgbMat_test)
dataB_xgbMat_test <- testing2xgMatrix
predicts_of_test_B_from_model_A <- predict(modelA, dataB_xgbMat_test)
df2 <- data.frame(predicts_of_test_B_from_model_A)
colnames(df2) <- c("predicts_of_train_B_from_model_A")
predictions_z <- predict(newLM, df2)
# ALIGNMNETS
alignXGB = function(modelA, dataB_xgbMat_test, dataB_xgbMat_train, dataB_train_labels, dataB_test_labels)
{
predicts_of_train_B_from_model_A <- predict(modelA, dataB_xgbMat_train)
df <- data.frame(dataB_train_labels, predicts_of_train_B_from_model_A)
newLM <- lm(dataB_train_labels ~ predicts_of_train_B_from_model_A, data = df)
predicts_of_test_B_from_model_A <- predict(modelA, dataB_xgbMat_test)
df2 <- data.frame(predicts_of_test_B_from_model_A)
colnames(df2) <- c("predicts_of_train_B_from_model_A")
predictions_z <- predict(newLM, df2)
residual <- dataB_test_labels - predictions_z
q <- quantile(residual_m2d1, probs =c(.025,.975))
c(newLM$coefficients,
sqrt(mean(((residual))^2)),
mean(abs(residual_m2d1)),
abs(q[1]) + abs(q[2]),
cor(dataB_test_labels, predictions_z))
}
alignXGB(bestModel, testing2xgMatrix, training2xgMatrix, training2RetentionTimesLabels, testing2DataLabels)
alignXGB(bestModel2, testingxgMatrix, trainingxgMatrix, trainingRetentionTimesLabels, testingDataLabels)
modelA <- bestModel2
dataB_xgbMat_test <- testingxgMatrix
dataB_xgbMat_train <- trainingxgMatrix
dataB_train_labels <- trainingRetentionTimesLabels
dataB_test_labels <- testingDataLabels
predicts_of_train_B_from_model_A <- predict(modelA, dataB_xgbMat_train)
df <- data.frame(dataB_train_labels, predicts_of_train_B_from_model_A)
newLM <- lm(dataB_train_labels ~ predicts_of_train_B_from_model_A, data = df)
predicts_of_test_B_from_model_A <- predict(modelA, dataB_xgbMat_test)
df2 <- data.frame(predicts_of_test_B_from_model_A)
colnames(df2) <- c("predicts_of_train_B_from_model_A")
predictions_z <- predict(newLM, df2)
residual <- dataB_test_labels - predictions_z
q <- quantile(residual_m2d1, probs =c(.025,.975))
c(newLM$coefficients,
sqrt(mean(((residual))^2)),
mean(abs(residual_m2d1)),
abs(q[1]) + abs(q[2]),
cor(dataB_test_labels, predictions_z))
# ALIGNMNETS
alignXGB = function(modelA, dataB_xgbMat_test, dataB_xgbMat_train, dataB_train_labels, dataB_test_labels)
{
predicts_of_train_B_from_model_A <- predict(modelA, dataB_xgbMat_train)
df <- data.frame(dataB_train_labels, predicts_of_train_B_from_model_A)
newLM <- lm(dataB_train_labels ~ predicts_of_train_B_from_model_A, data = df)
predicts_of_test_B_from_model_A <- predict(modelA, dataB_xgbMat_test)
df2 <- data.frame(predicts_of_test_B_from_model_A)
colnames(df2) <- c("predicts_of_train_B_from_model_A")
predictions_z <- predict(newLM, df2)
residual <- dataB_test_labels - predictions_z
q <- quantile(residual, probs =c(.025,.975))
c(newLM$coefficients,
sqrt(mean(((residual))^2)),
mean(abs(residual)),
abs(q[1]) + abs(q[2]),
cor(dataB_test_labels, predictions_z))
}
alignXGB(bestModel, testing2xgMatrix, training2xgMatrix, training2RetentionTimesLabels, testing2DataLabels)
alignXGB(bestModel2, testingxgMatrix, trainingxgMatrix, trainingRetentionTimesLabels, testingDataLabels)
library(xgboost)
set.seed(37)
# load data
train_data <- read.csv("data/clean/training.csv")
test_data <- read.csv("data/clean/testing.csv")
setwd("C:/Users/Kurtis/Desktop/TumorPrediction")
# load data
train_data <- read.csv("data/clean/training.csv")
test_data <- read.csv("data/clean/testing.csv")
library(xgboost)
set.seed(37)
# load data
train_data <- read.csv("data/clean/training.csv")
test_data <- read.csv("data/clean/testing.csv")
train_labels <- train_data$Outcome
train_data$Outcome <- NULL
xgb_train_data <- xgb.DMatrix(data.matrix(train_data), label = train_labels)
test_labels <- test_data$Outcome
test_data$Outcome <- NULL
xgb_test_data <- xgb.DMatrix(data.matrix(test_data), label = test_labels)
?xgb
library(xgboost)
?xgb
?xgboots
?xgboost
model <- xgb.cv(booster = "gbtree",
objective = "binary:logistic",
gamma = matrixToTry[row, 1],
child_weight = matrixToTry[row, 2],
max_depth = matrixToTry[row, 3],
subsample = matrixToTry[row, 4],
col_subsample = matrixToTry[row, 5],
eta = matrixToTry[row, 6],
nrounds = 10000,
nthreads = 28,
nfold = 5,
print_every_n = 2500,
early_stopping_rounds = 2,
data = xgb_train_data)
# a matrix to hold hyperparameter combinations
matrixToTry <- matrix(,nrow=0,ncol=6)
for (gamma in c(0, 0.1, 0.2, 0.3, 0.4))
{
for (child_weight in c(1,2,3,4,5,6))
{
for (col_subsample in c(0.8, 0.9, 1))
{
for (max_depth in c(9, 10, 11))
{
for (subsample in c(0.8, 0.9, 1))
{
for (eta in c(0.01, 0.05, 0.08))
{
matrixToTry <- rbind(matrixToTry,
c(gamma,child_weight,
max_depth, subsample, col_subsample,
eta))
}
}
}
}
}
}
row = 2
model <- xgb.cv(booster = "gbtree",
objective = "binary:logistic",
gamma = matrixToTry[row, 1],
child_weight = matrixToTry[row, 2],
max_depth = matrixToTry[row, 3],
subsample = matrixToTry[row, 4],
col_subsample = matrixToTry[row, 5],
eta = matrixToTry[row, 6],
nrounds = 10000,
nthreads = 28,
nfold = 5,
print_every_n = 2500,
early_stopping_rounds = 2,
data = xgb_train_data)
model <- xgb.cv(booster = "gbtree",
objective = "binary:logistic",
gamma = matrixToTry[row, 1],
child_weight = matrixToTry[row, 2],
max_depth = matrixToTry[row, 3],
subsample = matrixToTry[row, 4],
col_subsample = matrixToTry[row, 5],
eta = matrixToTry[row, 6],
nrounds = 10000,
nthreads = 28,
nfold = 5,
print_every_n = 2500,
early_stopping_rounds = 2,
data = xgb_train_data,
eval_metric = "log loss")
model <- xgb.cv(booster = "gbtree",
objective = "binary:logistic",
gamma = matrixToTry[row, 1],
child_weight = matrixToTry[row, 2],
max_depth = matrixToTry[row, 3],
subsample = matrixToTry[row, 4],
col_subsample = matrixToTry[row, 5],
eta = matrixToTry[row, 6],
nrounds = 10000,
nthreads = 28,
nfold = 5,
print_every_n = 2500,
early_stopping_rounds = 2,
data = xgb_train_data,
eval_metric = "logloss")
model <- xgb.cv(booster = "gbtree",
objective = "binary:logistic",
gamma = matrixToTry[row, 1],
child_weight = matrixToTry[row, 2],
max_depth = matrixToTry[row, 3],
subsample = matrixToTry[row, 4],
col_subsample = matrixToTry[row, 5],
eta = matrixToTry[row, 6],
nrounds = 10000,
nthreads = 28,
nfold = 5,
print_every_n = 2500,
early_stopping_rounds = 2,
data = xgb_train_data,
#eval_metric = "logloss"
)
model <- xgb.cv(booster = "gbtree",
objective = "binary:logistic",
gamma = matrixToTry[row, 1],
child_weight = matrixToTry[row, 2],
max_depth = matrixToTry[row, 3],
subsample = matrixToTry[row, 4],
col_subsample = matrixToTry[row, 5],
eta = matrixToTry[row, 6],
nrounds = 10000,
nthreads = 28,
nfold = 5,
print_every_n = 2500,
early_stopping_rounds = 2,
data = xgb_train_data,
#eval_metric = "logloss"
)
set.seed(37)
model <- xgb.cv(booster = "gbtree",
objective = "binary:logistic",
gamma = matrixToTry[row, 1],
child_weight = matrixToTry[row, 2],
max_depth = matrixToTry[row, 3],
subsample = matrixToTry[row, 4],
col_subsample = matrixToTry[row, 5],
eta = matrixToTry[row, 6],
nrounds = 10000,
nthreads = 28,
nfold = 5,
print_every_n = 2500,
early_stopping_rounds = 2,
data = xgb_train_data,
#eval_metric = "logloss"
)
set.seed(37)
model <- xgb.cv(booster = "gbtree",
objective = "binary:logistic",
gamma = matrixToTry[row, 1],
child_weight = matrixToTry[row, 2],
max_depth = matrixToTry[row, 3],
subsample = matrixToTry[row, 4],
col_subsample = matrixToTry[row, 5],
eta = matrixToTry[row, 6],
nrounds = 10000,
nthreads = 28,
nfold = 5,
print_every_n = 2500,
early_stopping_rounds = 2,
data = xgb_train_data,
#eval_metric = "logloss"
)
set.seed(37)
model <- xgb.cv(booster = "gbtree",
objective = "binary:logistic",
gamma = matrixToTry[row, 1],
child_weight = matrixToTry[row, 2],
max_depth = matrixToTry[row, 3],
subsample = matrixToTry[row, 4],
col_subsample = matrixToTry[row, 5],
eta = matrixToTry[row, 6],
nrounds = 10000,
nthreads = 28,
nfold = 5,
print_every_n = 2500,
early_stopping_rounds = 2,
data = xgb_train_data,
eval_metric = "logloss"
)
model$call
model$params
model$best_iteration
model$params
model$evaluation_log
model$evaluation_log[model$best_iteration]
set.seed(37)
model <- xgb.cv(booster = "gbtree",
objective = "binary:logistic",
gamma = matrixToTry[row, 1],
child_weight = matrixToTry[row, 2],
max_depth = matrixToTry[row, 3],
subsample = matrixToTry[row, 4],
col_subsample = matrixToTry[row, 5],
eta = matrixToTry[row, 6],
nrounds = 10000,
nthreads = 28,
nfold = 5,
print_every_n = 2500,
early_stopping_rounds = 2,
data = xgb_train_data,
eval_metric = "error"
)
row = 4
set.seed(37)
model <- xgb.cv(booster = "gbtree",
objective = "binary:logistic",
gamma = matrixToTry[row, 1],
child_weight = matrixToTry[row, 2],
max_depth = matrixToTry[row, 3],
subsample = matrixToTry[row, 4],
col_subsample = matrixToTry[row, 5],
eta = matrixToTry[row, 6],
nrounds = 10000,
nthreads = 28,
nfold = 5,
print_every_n = 2500,
early_stopping_rounds = 2,
data = xgb_train_data,
eval_metric = "error"
)
model <- xgb.cv(booster = "gbtree",
objective = "binary:logistic",
gamma = matrixToTry[row, 1],
child_weight = matrixToTry[row, 2],
max_depth = matrixToTry[row, 3],
subsample = matrixToTry[row, 4],
col_subsample = matrixToTry[row, 5],
eta = matrixToTry[row, 6],
nrounds = 10000,
nthreads = 28,
nfold = 5,
print_every_n = 2500,
early_stopping_rounds = 2,
data = xgb_train_data,
eval_metric = "logloss"
)
row = 4
set.seed(37)
model <- xgb.cv(booster = "gbtree",
objective = "binary:logistic",
gamma = matrixToTry[row, 1],
child_weight = matrixToTry[row, 2],
max_depth = matrixToTry[row, 3],
subsample = matrixToTry[row, 4],
col_subsample = matrixToTry[row, 5],
eta = matrixToTry[row, 6],
nrounds = 10000,
nthreads = 28,
nfold = 5,
print_every_n = 2500,
early_stopping_rounds = 2,
data = xgb_train_data,
eval_metric = "logloss"
)
