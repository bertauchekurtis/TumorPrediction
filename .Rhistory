youden_index = numeric())
calcMetrics <- function(confusion_matrix, predictions, labels)
{
accuracy <- confusion_matrix$overall[1]
sensitivity <- confusion_matrix$byClass[1]
specificity <- confusion_matrix$byClass[2]
false_positive_rate <- (confusion_matrix$table[2]/(confusion_matrix$table[2] + confusion_matrix$table[1]))
false_negative_rate <- (confusion_matrix$table[3]/(confusion_matrix$table[4]+confusion_matrix$table[3]))
positive_predictive_value <- confusion_matrix$byClass[3]
negative_predictive_value <- confusion_matrix$byClass[4]
positive_likelihood_ratio <- sensitivity / false_positive_rate
negative_likelihood_ratio <- false_negative_rate / specificity
diagnostic_odds_ratio <- positive_likelihood_ratio / negative_likelihood_ratio
f1_score <- (1 + 1^2) * ((sensitivity*positive_predictive_value)/(((1^2)*positive_predictive_value)+sensitivity))
f2_score <- (1 + 2^2) * ((sensitivity*positive_predictive_value)/(((2^2)*positive_predictive_value)+sensitivity))
youden_index <- sensitivity + specificity - 1
pred <- prediction(predictions, labels)
auc_perf <- performance(pred, measure = "auc")
area_under_roc_curve <- auc_perf@y.values[[1]]
returnFrame<- data.frame(accuracy = c(accuracy),
sensitivity = c(sensitivity),
specificity = c(specificity),
false_positive_rate = c(false_positive_rate),
false_negative_rate = c(false_negative_rate),
positive_predictive_value = c(positive_predictive_value),
negative_predictive_value = c(negative_predictive_value),
area_under_roc_curve = c(area_under_roc_curve),
positive_likelihood_ratio = c(positive_likelihood_ratio),
negative_likelihood_ratio = c(negative_likelihood_ratio),
diagnostic_odds_ratio = c(diagnostic_odds_ratio),
f1_score = c(f1_score),
f2_score = c(f2_score),
youden_index = c(youden_index))
return(returnFrame)
}
load("models/lasso_model.RData")
load("results/lasso_confusion_matrix.RData")
test_data_glm_matrix <- model.matrix(Outcome ~ .,
test_data)[,-1]
lasso_predictions <- predict(lasso_model,
newx = test_data_glm_matrix,
type = "response")
hm <- calcMetrics(lasso_confusion_matrix,
lasso_predictions,
test_data$Outcome)
View(hm)
performanceMetricFrame <- rbind(performanceMetricFrame, hm)
View(performanceMetricFrame)
load("models/ridge_model.RData")
load("results/ridge_confusion_matrix.RData")
ridge_predictions <- predict(ridge_model,
newx = test_data_glm_matrix,
type = "response")
performanceMetricFrame <- rbind(performanceMetricFrame,
calcMetrics(ridge_confusion_matrix,
ridge_predictions,
test_data$Outcome))
lasso_confusion_matrix
ridge_confusion_matrix
load("models/elastic_net_model.RData")
load("restuls/elastic_net_confusion_matrix.RData")
elastic_predictions <- predict(elastic_net_model,
newx = test_data_glm_matrix,
type = "response")
load("models/elastic_net_model.RData")
load("results/elastic_net_confusion_matrix.RData")
elastic_predictions <- predict(elastic_net_model,
newx = test_data_glm_matrix,
type = "response")
performanceMetricFrame <- rbind(performanceMetricFrame,
calcMetrics(elastic_net_confusion_matrix,
elastic_predictions,
test_data$Outcome))
# add linear svm
load("models/best_linear_svm_model.RData")
load("results/linear_svm_confusion_matrix.RData")
linear_svm_predictions <- predict(best_linear_svm_model,
test_data_glm_matrix)
linear_svm_predictions <- predict(best_linear_svm,
test_data_glm_matrix)
linear_svm_predictions <- predict(best_linear_svm,
test_data)
linear_svm_predictions <- predict(best_linear_svm,
test_data)
library(e1071)
linear_svm_predictions <- predict(best_linear_svm,
test_data)
linear_svm_predictions
linear_svm_predictions <- predict(best_linear_svm,
test_data,
type = "response")
linear_svm_predictions
?predict
linear_svm_predictions <- predict(best_linear_svm,
test_data,
probability = TRUE)
best_linear_svm <- svm(as.factor(Outcome) ~ .,
data = train_data,
kernel = "linear",
probability = TRUE,
cost = linear_svm$bestTune)
# load data
train_data <- read.csv("data/clean/training.csv")
set.seed(37)
# libraries
library(e1071)
library(caret)
set.seed(37)
# load data
train_data <- read.csv("data/clean/training.csv")
test_data <- read.csv("data/clean/testing.csv")
train_control <- trainControl(method = "cv", number = 5)
# LINEAR SVM
linear_svm <- train(as.factor(Outcome) ~.,
data = train_data,
method = "svmLinear",
trControl = train_control,
tuneGrid = expand.grid(C = seq(0.01, 2, length = 20)))
warnings()
best_linear_svm <- svm(as.factor(Outcome) ~ .,
data = train_data,
kernel = "linear",
probability = TRUE,
cost = linear_svm$bestTune)
linear_svm_predictions <- predict(best_linear_svm,
test_data)
linear_svm_predictions
m
linear_svm_confusion_matrix <- confusionMatrix(data = factor(linear_svm_predictions),
reference = factor(test_data$Outcome),
positive = "1")
save(best_linear_svm, file = "models/best_linear_svm_model.RData")
save(linear_svm_confusion_matrix, file = "results/linear_svm_confusion_matrix.RData")
# RADIAL KERNEL
raidal_svm <- train(as.factor(Outcome) ~.,
data = train_data,
method = "svmRadial",
trControl = train_control,
tuneLength = 20)
best_radial_svm <- svm(as.factor(Outcome) ~ .,
data = train_data,
kernel = "radial",
probability = TRUE,
cost = raidal_svm$bestTune[2],
gamma = raidal_svm$bestTune[1])
radial_svm_predictions <- predict(best_radial_svm,
test_data)
radial_svm_confusion_matrix <- confusionMatrix(data = factor(radial_svm_predictions),
reference = factor(test_data$Outcome),
positive = "1")
save(best_radial_svm, file = "models/best_radial_svm_model.RData")
save(radial_svm_confusion_matrix, file = "results/radial_svm_confusion_matrix.RData")
# add linear svm
load("models/best_linear_svm_model.RData")
load("results/linear_svm_confusion_matrix.RData")
linear_svm_predictions <- predict(best_linear_svm,
test_data,
probability = TRUE)
linear_svm_predictions
linear_svm_predictions[1]
linear_svm_predictions[2]
linear_svm_predictions[[3]]
m <- attr(linear_svm_predictions, "probabilities")
m
linear_svm_confusion_matrix
linear_svm_predictions <- attr(linear_svm_predictions, "probabilities")
linear_svm_predictions
linear_svm_predictions[1]
linear_svm_predictions <- ifelse(linear_svm_predictions[1] > linear_svm_predictions[2],
linear_svm_predictions[1],
linear_svm_predictions[2])
linear_svm_predictions
linear_svm_predictions <- predict(best_linear_svm,
test_data,
probability = TRUE)
linear_svm_predictions <- attr(linear_svm_predictions, "probabilities")
linear_svm_predictions
linear_svm_predictions[,1]
linear_svm_predictions <- ifelse(linear_svm_predictions[,1] > linear_svm_predictions[,2],
linear_svm_predictions[,1],
linear_svm_predictions[,2])
linear_svm_predictions
performanceMetricFrame <- rbind(performanceMetricFrame,
calcMetrics(linear_svm_confusion_matrix,
linear_svm_predictions,
test_data$Outcome))
linear_svm_predictions <- predict(best_linear_svm,
test_data,
probability = TRUE)
linear_svm_predictions <- attr(linear_svm_predictions, "probabilities")
linear_svm_predictions <- linear_svm_predictions[,1]
performanceMetricFrame <- rbind(performanceMetricFrame,
calcMetrics(linear_svm_confusion_matrix,
linear_svm_predictions,
test_data$Outcome))
linear_svm_predictions <- predict(best_linear_svm,
test_data,
probability = TRUE)
linear_svm_predictions <- attr(linear_svm_predictions, "probabilities")
linear_svm_predictions <- linear_svm_predictions[,2]
performanceMetricFrame <- rbind(performanceMetricFrame,
calcMetrics(linear_svm_confusion_matrix,
linear_svm_predictions,
test_data$Outcome))
# add radial svm
load("models/best_radial_svm_model.RData")
load("results/radial_svm_confusion_matrix.RData")
radial_svm_predictions <- predict(best_radial_svm.RData,
test_data,
probability = TRUE)
radial_svm_predictions <- predict(best_radial_svm,
test_data,
probability = TRUE)
radial_svm_predictions <- attr(radial_svm_predictions, "probabilities")
radial_svm_predictions <- radial_svm_predictions[,2]
performanceMetricFrame <- rbind(performanceMetricFrame,
calcMetrics(radial_svm_confusion_matrix,
radial_svm_predictions,
test_data$Outcome))
performanceMetricFrame <- rbind(performanceMetricFrame,
calcMetrics(linear_svm_confusion_matrix,
linear_svm_predictions,
test_data$Outcome)
j
radial_svm_confusion_matrix
# add rf
load("models/random_forest_model.RData")
load("results/random_forest_confusion_matrix.RData")
rf_predictions <- predict(random_forest_model,
test_data)
library(ranger)
rf_predictions <- predict(random_forest_model,
test_data)
View(rf_predictions)
rf_predictions <- predict(random_forest_model,
test_data,
type = "response")
View(rf_predictions)
rf_predictions
rf_predictions$predictions
rf_predictions <- predict(random_forest_model,
test_data,
probability = TRUE)
rf_predictions$predictions
rf_predictions
rf_predictions$treetype
rf_predictions <- predict(random_forest_model,
test_data,
type = "prob")
rf_predictions <- predict(random_forest_model,
test_data,
type = "probability")
?predict.ranger.forest
# libraries
library(ranger)
set.seed(37)
# load data
train_data <- read.csv("data/clean/training.csv")
test_data <- read.csv("data/clean/testing.csv")
# hold all the hyperparameters we wnat to try
hyperParameter_matrix <- matrix(,nrow=0,ncol=2)
random_forest_model <- NULL
random_forest_prediction_error <- 1
for(numTrees in c(100, 200, 300, 400, 500, 600, 750, 1000, 2000, 3000, 5000, 10000))
{
for(mtry in c(100, 250, 500, 1000, 2000, 3000, 3500, 3600, 3700, 3800, 3900, 4000, 4200, 4400))
{
hyperParameter_matrix <- rbind(hyperParameter_matrix, c(numTrees, mtry))
}
}
# tune the model
for(row in 1:nrow(hyperParameter_matrix))
{
set.seed(37)
rfModel <- ranger(formula = Outcome ~.,
data = train_data,
num.trees = hyperParameter_matrix[row,1],
mtry = hyperParameter_matrix[row,2],
min.node.size = 5,
num.threads = 24,
classification = TRUE,
probability = TRUE)
if(rfModel$prediction.error < random_forest_prediction_error)
{
random_forest_prediction_error <- rfModel$prediction.error
random_forest_model <- rfModel
save(random_forest_model, file = "models/random_forest_model.RData")
print("New Best Model Found. Prediction Error:")
print(random_forest_prediction_error)
print("Num Trees:")
print(hyperParameter_matrix[row,1])
print("MTRY:")
print(hyperParameter_matrix[row,2])
}
rm(rfModel)
}
# continue tuning based on initial observations
hyperParameter_matrix <- matrix(,nrow=0,ncol=2)
random_forest_model <- NULL
random_forest_prediction_error <- 1 # why restart? there are simpler options that might have the same results as the already previous model, and we prefer a simpler model
for(numTrees in c(100, 200, 300))
{
for(mtry in c(3000, 3050, 3100, 3150, 3200, 3250, 3300, 3350, 3400, 3450, 3500))
{
hyperParameter_matrix <- rbind(hyperParameter_matrix, c(numTrees, mtry))
}
}
for(row in 1:nrow(hyperParameter_matrix))
{
set.seed(37)
rfModel <- ranger(formula = Outcome ~.,
data = train_data,
num.trees = hyperParameter_matrix[row,1],
mtry = hyperParameter_matrix[row,2],
min.node.size = 5,
num.threads = 24,
classification = TRUE,
probability = TRUE)
if(rfModel$prediction.error < random_forest_prediction_error)
{
random_forest_prediction_error <- rfModel$prediction.error
random_forest_model <- rfModel
save(random_forest_model, file = "models/random_forest_model.RData")
print("New Best Model Found. Prediction Error:")
print(random_forest_prediction_error)
print("Num Trees:")
print(hyperParameter_matrix[row,1])
print("MTRY:")
print(hyperParameter_matrix[row,2])
}
rm(rfModel)
}
# evaluate best model
random_forest_predictions <- predict(random_forest_model,
test_data)
random_forest_predictions <- random_forest_predictions$predictions
random_forest_predictions
random_forest_predictions <- random_forest_predictions[,2]
random_forest_predictions <- ifelse(random_forest_predictions > 0.5,
1,
0)
random_forest_predictions
random_forest_confusion_matrix <- confusionMatrix(data = factor(random_forest_predictions),
reference = factor(test_data$Outcome),
positive = "1")
random_forest_confusion_matrix\
random_forest_confusion_matrix
save(random_forest_confusion_matrix, file = "results/random_forest_confusion_matrix.RData")
# add rf
load("models/random_forest_model.RData")
load("results/random_forest_confusion_matrix.RData")
rf_predictions <- predict(random_forest_model,
test_data)
rf_predictions
rf_predictions$predictions
rf_predictions$predictions[,2]
rf_predictions <- rf_predictions$predictions[,2]
performanceMetricFrame <- rbind(performanceMetricFrame,
calcMetrics(random_forest_confusion_matrix,
rf_predictions,
test_data$Outcome))
# and add xgb
load("models/best_xgb_model_full.RData")
load("results/xgb_confusion_matrix.RData")
library(xgboost)
test_labels <- test_data$Outcome
test_data$Outcome <- NULL
xgb_test_data <- xgb.DMatrix(data.matrix(test_data), label = test_labels)
xgb_predictions <- predict(best_xgb_model_full,
xgb_test_data)
performanceMetricFrame <- rbind(performanceMetricFrame,
calcMetrics(xgb_confusion_matrix,
xgb_predictions,
test_labels))
library(ROCR)
library(glmnet)
library(e1071)
library(xgboost)
library(ranger)
test_data <- read.csv("data/clean/testing.csv")
# a frame to hold everything
performanceMetricFrame <- data.frame(accuracy = numeric(),
sensitivity = numeric(),
specificity = numeric(),
false_positive_rate = numeric(),
false_negative_rate = numeric(),
positive_predictive_value = numeric(),
negative_predictive_value = numeric(),
area_under_roc_curve = numeric(),
positive_likelihood_ratio = numeric(),
negative_likelihood_ratio = numeric(),
diagnostic_odds_ratio = numeric(),
f1_score = numeric(),
f2_score = numeric(),
youden_index = numeric())
# a function to calc metrics
calcMetrics <- function(confusion_matrix, predictions, labels)
{
accuracy <- confusion_matrix$overall[1]
sensitivity <- confusion_matrix$byClass[1]
specificity <- confusion_matrix$byClass[2]
false_positive_rate <- (confusion_matrix$table[2]/(confusion_matrix$table[2] + confusion_matrix$table[1]))
false_negative_rate <- (confusion_matrix$table[3]/(confusion_matrix$table[4]+confusion_matrix$table[3]))
positive_predictive_value <- confusion_matrix$byClass[3]
negative_predictive_value <- confusion_matrix$byClass[4]
positive_likelihood_ratio <- sensitivity / false_positive_rate
negative_likelihood_ratio <- false_negative_rate / specificity
diagnostic_odds_ratio <- positive_likelihood_ratio / negative_likelihood_ratio
f1_score <- (1 + 1^2) * ((sensitivity*positive_predictive_value)/(((1^2)*positive_predictive_value)+sensitivity))
f2_score <- (1 + 2^2) * ((sensitivity*positive_predictive_value)/(((2^2)*positive_predictive_value)+sensitivity))
youden_index <- sensitivity + specificity - 1
pred <- prediction(predictions, labels)
auc_perf <- performance(pred, measure = "auc")
area_under_roc_curve <- auc_perf@y.values[[1]]
returnFrame<- data.frame(accuracy = c(accuracy),
sensitivity = c(sensitivity),
specificity = c(specificity),
false_positive_rate = c(false_positive_rate),
false_negative_rate = c(false_negative_rate),
positive_predictive_value = c(positive_predictive_value),
negative_predictive_value = c(negative_predictive_value),
area_under_roc_curve = c(area_under_roc_curve),
positive_likelihood_ratio = c(positive_likelihood_ratio),
negative_likelihood_ratio = c(negative_likelihood_ratio),
diagnostic_odds_ratio = c(diagnostic_odds_ratio),
f1_score = c(f1_score),
f2_score = c(f2_score),
youden_index = c(youden_index))
return(returnFrame)
}
# add lasso to data frame of metrics
load("models/lasso_model.RData")
load("results/lasso_confusion_matrix.RData")
test_data_glm_matrix <- model.matrix(Outcome ~ .,
test_data)[,-1]
lasso_predictions <- predict(lasso_model,
newx = test_data_glm_matrix,
type = "response")
performanceMetricFrame <- rbind(performanceMetricFrame,
calcMetrics(lasso_confusion_matrix,
lasso_predictions,
test_data$Outcome))
# add ridge
load("models/ridge_model.RData")
load("results/ridge_confusion_matrix.RData")
ridge_predictions <- predict(ridge_model,
newx = test_data_glm_matrix,
type = "response")
performanceMetricFrame <- rbind(performanceMetricFrame,
calcMetrics(ridge_confusion_matrix,
ridge_predictions,
test_data$Outcome))
# add elastic
load("models/elastic_net_model.RData")
load("results/elastic_net_confusion_matrix.RData")
elastic_predictions <- predict(elastic_net_model,
newx = test_data_glm_matrix,
type = "response")
performanceMetricFrame <- rbind(performanceMetricFrame,
calcMetrics(elastic_net_confusion_matrix,
elastic_predictions,
test_data$Outcome))
# add linear svm
load("models/best_linear_svm_model.RData")
load("results/linear_svm_confusion_matrix.RData")
linear_svm_predictions <- predict(best_linear_svm,
test_data,
probability = TRUE)
linear_svm_predictions <- attr(linear_svm_predictions, "probabilities")
linear_svm_predictions <- linear_svm_predictions[,2]
performanceMetricFrame <- rbind(performanceMetricFrame,
calcMetrics(linear_svm_confusion_matrix,
linear_svm_predictions,
test_data$Outcome))
# add radial svm
load("models/best_radial_svm_model.RData")
load("results/radial_svm_confusion_matrix.RData")
radial_svm_predictions <- predict(best_radial_svm,
test_data,
probability = TRUE)
radial_svm_predictions <- attr(radial_svm_predictions, "probabilities")
radial_svm_predictions <- radial_svm_predictions[,2]
performanceMetricFrame <- rbind(performanceMetricFrame,
calcMetrics(radial_svm_confusion_matrix,
radial_svm_predictions,
test_data$Outcome))
# add rf
load("models/random_forest_model.RData")
load("results/random_forest_confusion_matrix.RData")
rf_predictions <- predict(random_forest_model,
test_data)
rf_predictions <- rf_predictions$predictions[,2]
performanceMetricFrame <- rbind(performanceMetricFrame,
calcMetrics(random_forest_confusion_matrix,
rf_predictions,
test_data$Outcome))
# and add xgb
load("models/best_xgb_model_full.RData")
load("results/xgb_confusion_matrix.RData")
test_labels <- test_data$Outcome
test_data$Outcome <- NULL
xgb_test_data <- xgb.DMatrix(data.matrix(test_data), label = test_labels)
xgb_predictions <- predict(best_xgb_model_full,
xgb_test_data)
performanceMetricFrame <- rbind(performanceMetricFrame,
calcMetrics(xgb_confusion_matrix,
xgb_predictions,
test_labels))
rownames(performanceMetricFrame) <- ("lasso","ridge","elastic_net","linear_svm","radial_svm","random_forest","xgb")
rownames(performanceMetricFrame) <- c("lasso","ridge","elastic_net","linear_svm","radial_svm","random_forest","xgb")
performanceMetricFrame <- t(performanceMetricFrame)
save(performanceMetricFrame, file = "results/performanceMetricFrame.RData")
